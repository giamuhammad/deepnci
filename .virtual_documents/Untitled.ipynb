import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import glob



# -------------------------------
# DATASET
# -------------------------------
class NCIDataset(Dataset):
    def __init__(self, df, density_dir, descriptor_cols, label_col="target_nci", scaler=None):
        self.df = df.reset_index(drop=True)
        self.density_dir = density_dir
        self.descriptor_cols = descriptor_cols
        self.label_col = label_col

        # Normalize descriptors
        if scaler == None:
            self.scaler = StandardScaler()
        else:
            self.scaler = scaler
        self.descriptors = self.scaler.fit_transform(df[descriptor_cols].values)
        self.labels = df[label_col].values.astype(np.float32)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        density_path = os.path.join(self.density_dir, row["filename"] + ".npy")
        density = np.load(density_path).astype(np.float32)
        density = np.expand_dims(density, axis=0)  # (1, D, H, W)
        desc = self.descriptors[idx].astype(np.float32)
        label = self.labels[idx]
        return torch.tensor(density), torch.tensor(desc), torch.tensor(label)

# -------------------------------
# MODELS
# -------------------------------
class EDNCI(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv3d(1, 32, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool3d(2)
        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool3d(2)
        self.dropout = nn.Dropout3d(0.3)
        # After two pools: (137→68→34, 133→66→33, 124→62→31) → ~34*33*31*64 ≈ 2.2M
        self.fc = nn.Linear(64 * 34 * 33 * 31, 128)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool1(x)
        x = torch.relu(self.conv2(x))
        x = self.pool2(x)
        x = self.dropout(x)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc(x))
        return x

class CPNCI(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.fc = nn.Linear(input_dim, 10)

    def forward(self, x):
        return torch.relu(self.fc(x))

class DeepNCI(nn.Module):
    def __init__(self, desc_dim):
        super().__init__()
        self.ednci = EDNCI()
        self.cpnci = CPNCI(desc_dim)
        self.fusion = nn.Sequential(
            nn.Linear(128 + 10, 64),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # regression output
        )

    def forward(self, density, descriptors):
        f1 = self.ednci(density)
        f2 = self.cpnci(descriptors)
        fused = torch.cat([f1, f2], dim=1)
        out = self.fusion(fused)
        return out.squeeze(-1)



BATCH_SIZE = 8
LEARNING_RATE = 1e-4
EPOCHS = 200
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Sesuaikan path
DENSITY_NPY_DIR = "./density_npy"
DESCRIPTORS_CSV = "./molecular_properties_results.csv"  # Kolom: filename (tanpa .npy), dft_nci, prop1, prop2, ..., target_nci
TARGET_NCI = "./target.csv"
TARGET_GRID = (137, 133, 124)



# Load descriptor + label data
df = pd.read_csv(DESCRIPTORS_CSV)
df.head()


target = pd.read_csv(TARGET_NCI)
target.head()





X = df.drop(['molecule_name', 'num_atoms'], axis=1) # axis=1 specifies dropping a column
Y = target.drop(target.columns[0], axis=1)


data = [X, Y]
data = pd.concat(data, axis=1)
data.head()


# Assume df has column 'filename' matching .npy names (without extension)
# Descriptor columns (exclude filename, target_nci, but include DFT NCI as primary descriptor!)
DESC_COLS = [col for col in data.columns if col not in ["filename", "ccbd"]]
DESC_COLS
# Ensure 'dft_nci' is included — it's critical per paper

# Split train/val/test (80/10/10)
#train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)
#val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)
#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)
train, test = train_test_split(data, test_size=0.3, random_state=42)

train_dataset = NCIDataset(train, DENSITY_NPY_DIR, DESC_COLS, label_col="ccbd")
test_dataset = NCIDataset(test, DENSITY_NPY_DIR, DESC_COLS, label_col="ccbd", scaler=train_dataset.scaler)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)




model = DeepNCI(desc_dim=len(DESC_COLS)).to(DEVICE)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-8)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)

best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for density, desc, target in train_loader:
        density, desc, target = density.to(DEVICE), desc.to(DEVICE), target.to(DEVICE)
        optimizer.zero_grad()
        pred = model(density, desc)
        loss = criterion(pred, target)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    scheduler.step()

    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for density, desc, target in val_loader:
            density, desc, target = density.to(DEVICE), desc.to(DEVICE), target.to(DEVICE)
            pred = model(density, desc)
            val_loss += criterion(pred, target).item()

    train_loss /= len(train_loader)
    val_loss /= len(val_loader)

    print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), "best_deepnci.pth")
        print(" → Saved best model")

print("Training selesai. Model terbaik disimpan sebagai 'best_deepnci.pth'")



